针对之前回答中提到的不同技术路线，我为你整理了该领域最经典、最核心的**基石论文（State-of-the-Art Papers）**。如果你想深入研究事件相机 SLAM，这份书单是必读的。

以下按照技术路线分类：

### 0. 综述（入门必读）
如果你只需要读一篇，请读这篇。它是目前的“圣经”级综述，涵盖了 SLAM、重建、识别等所有领域。
*   **论文题目:** **Event-based Vision: A Survey**
*   **作者:** Guillermo Gallego, Tobi Delbruck, et al.
*   **发表:** IEEE TPAMI 2020
*   **贡献:** 详细解释了事件相机的工作原理、噪声模型、以及截止 2020 年所有的主流算法分类（Filter vs Optimization）。

---

### 1. 基于“对比度最大化”的方法 (Optimization-based)
这是我在上一个回答中提到的核心概念，也是目前最优雅的数学解释。

*   **核心理论论文:** **Focus Is All You Need: Loss Functions for Event-based Vision**
    *   **作者:** Guillermo Gallego, Henri Rebecq, Davide Scaramuzza
    *   **发表:** CVPR 2019
    *   **贡献:** 提出了“对比度最大化”（Contrast Maximization）框架。证明了只要找到正确的运动轨迹，将事件叠加在一起就能得到最清晰的图像。这是现代事件 SLAM 的数学基础。

*   **双目 SLAM 代表作:** **ESVO: A Real-time Event-based Stereo Visual Odometry**
    *   **作者:** Yi Zhou, Guillermo Gallego, Shaojie Shen (沈劭劼团队)
    *   **发表:** IEEE Transactions on Robotics (TRO) 2021
    *   **贡献:** 也就是前文提到的 **ESVO**。它利用双目事件相机，结合了时空立体匹配和非线性优化，实现了完全运行在 CPU 上的实时 SLAM，代码开源且非常流行。

---

### 2. 混合融合方法 (Event + Frame + IMU)
由于事件相机在静止时“失明”，目前最稳健的系统通常结合了传统图像。

*   **代表作:** **Ultimate SLAM? Combining Events, Images, and IMU for Robust Visual SLAM in HDR and High-Speed Scenarios**
    *   **作者:** Antoni Rosinol, Henri Rebecq, Davide Scaramuzza
    *   **发表:** IEEE Robotics and Automation Letters (RA-L) 2018
    *   **贡献:** 提出了 **Ultimate SLAM**。它通过紧耦合（Tightly-coupled）的方式融合了事件、标准帧和 IMU。证明了在高速运动或高动态范围（HDR）场景下，这种组合比单纯的传统 SLAM 强得多。

---

### 3. 基于滤波的方法 (Filter-based / VIO)
这是早期将事件相机用于 VIO（视觉惯性里程计）的主流方法，侧重于概率更新。

*   **代表作:** **Event-based Visual Inertial Odometry**
    *   **作者:** Alex Zihao Zhu, Nikolay Atanasov, Kostas Daniilidis
    *   **发表:** CVPR 2017
    *   **贡献:** 也就是著名的 **EVIO**。它将事件作为特征观测值，放入 MSCKF（多状态约束卡尔曼滤波）框架中。这是最早证明事件相机可以做鲁棒 VIO 的论文之一。

---

### 4. 基于几何重建的方法 (Geometric / Tracking and Mapping)
试图通过几何方法构建 3D 地图并进行跟踪。

*   **代表作:** **EVO: A Geometric Approach to Event-Based 6-DOF Parallel Tracking and Mapping**
    *   **作者:** Henri Rebecq, Timo Horstschaefer, Davide Scaramuzza
    *   **发表:** IEEE RA-L 2016
    *   **贡献:** 提出了 **EVO** 算法。它采用了类似于传统 PTAM（Parallel Tracking and Mapping）的架构，但在后端使用了“EMVS”（Event-based Multi-View Stereo）来进行半稠密建图。

---

### 5. 基于深度学习的方法 (Deep Learning)
利用神经网络解决“去噪”和“位姿估计”问题。

*   **自监督学习:** **Unsupervised Event-based Learning of Optical Flow, Depth, and Egomotion**
    *   **作者:** Alex Zihao Zhu, et al.
    *   **发表:** CVPR 2019
    *   **贡献:** 提出了一种无监督的学习框架，利用光度一致性作为损失函数，训练网络从事件流中直接预测深度和相机运动，不需要真值（Ground Truth）。

*   **视频重建作为前端:** **E2VID: High Quality Event-to-Video Reconstruction**
    *   **作者:** Henri Rebecq, et al.
    *   **发表:** CVPR 2019
    *   **贡献:** 虽然主要讲的是如何把事件变成清晰的视频（E2VID），但这个网络常被用作 SLAM 的前端——先把事件变成高质量视频，然后直接扔给 ORB-SLAM 或 VINS-Mono 跑。

### 建议阅读顺序
1.  **Survey (Gallego 2020)** - 建立全局认知。
2.  **Focus Is All You Need (2019)** - 理解核心数学原理。
3.  **ESVO (2021)** 或 **Ultimate SLAM (2018)** - 学习如何构建一个完整的 SLAM 系统。

这些论文的大部分作者都来自 **苏黎世大学 (UZH) 的 RPG 实验室 (Davide Scaramuzza 教授)**，他们是乃至全球事件相机 SLAM 领域的领头羊，你可以重点关注该实验室的后续产出。