https://arxiv.org/pdf/2502.11089v1
 提出了一种名为 NSA（Native Sparse Attention）的新型稀疏注意力机制。主要讨论了如何设计一个既能在训练时端到端优化、又能在推理阶段显著提高效率的注意力机制，以应对长文本的处理挑战。ds他们设计了一种分层的稀疏策略，将注意力分为三个分支：压缩（compression）、选择（selection）和滑动窗口（sliding window），以便同时捕捉全局上下文和局部精细信息。
- 压缩
 这一分支将连续的一段 token 序列按照固定块大小进行划分，并对每个块内的所有 token 进行聚合。通过一个带有位置编码的可学习 MLP，将一个块内的信息融合成一个单一的“压缩”表示。（这个MLP可以降低语义损失，尽量保留表征）这种方式捕捉了块级的全局语义信息，并大幅减少了后续注意力计算所需处理的 token 数量。
- 选择
 为了防止压缩过程中丢失重要的细粒度信息，选择分支在较粗略的压缩之外，专门挑选出最具信息量的 token 块。具体方法是：先将序列按连续块划分，然后为每个块计算一个重要性得分（通常基于与查询的注意力得分），接着选取得分最高的几个块，这些块内的 token 将被保留下来用于精细化的注意力计算。这种策略保证了模型在保留全局结构信息的同时，也能捕捉到局部的关键信息。这个像啥，特别像他们玩的token wised quantized
- 滑动窗口
 除了压缩和选择外，滑动窗口分支专注于捕捉查询 token 与其相邻 token 之间的局部关系。该分支在输入序列中维护一个固定大小的窗口，对窗口内的 token 进行常规的注意力计算，确保模型能敏感地捕捉到近邻之间的细节和依赖关系，从而防止在全局稀疏化处理时局部信息被遗漏，这个就没什么特别可讲得了