[[826历年真题解析-数据结构篇（包括算法题）-12.15.pdf#page=34&rect=14,303,586,809|826历年真题解析-数据结构篇（包括算法题）-12.15, p.34]]
![[Pasted image 20251215135336.png]]

- 散列表（Hash Table）的构建以及解决冲突的一种特定方法—— #双散列法  
1. 基础设定     
	*   **表长（$m$）**：13   [[散列函数Hash哈希函数]] 
	*   **第一散列函数**：$H(key)=key\%13$（用于计算初始位置） #除留余数法  
	*   **第二散列函数**：$H'(key)=(7\times key\%10)+1$（用于计算发生冲突时的“步长”）
	*   **双散列探测公式**：当发生冲突时，第$i$次探测的位置为 $H_i=(H(key)+i\times H'(key))\%m$。其中$i=1,2,3...$。
2. 逐个插入推导 
	1. 将输入序列 $12,23,45,57,20,3,78,31,15,36$ 依次插入 
		1. 无冲突的直接插入（查找长度均为1）
		    *   **12**: $12\%13=12$。位置12为空，放入。
		    *   **23**: $23\%13=10$。位置10为空，放入。
		    *   **45**: $45\%13=6$。位置6为空，放入。
		    *   **57**: $57\%13=5$。位置5为空，放入。
		    *   **20**: $20\%13=7$。位置7为空，放入。
		    *   **3**: $3\%13=3$。位置3为空，放入。
		    *   **78**: $78\%13=0$。位置0为空，放入。
		    *   **15**: $15\%13=2$。位置2为空，放入。
	2. 发生冲突的插入（重点）
		  - **31的插入过程**：
	        1.  **计算初始位置**：$H(31)=31\%13=5$。位置5已被57占用，**冲突**。
	        2.  **计算步长**：$H'(31)=(7\times31\%10)+1=(217\%10)+1=7+1=8$。
	        3.  **第1次探测**：$(5+1\times8)\%13=13\%13=0$。位置0已被78占用，**冲突**。
	        4.  **第2次探测**：$(5+2\times8)\%13=(5+16)\%13=21\%13=8$。位置8为空，**放入**。
	        *   **查找次数**：初始1次 + 探测2次 = **3次**。
		- **36的插入过程**：
	        1.  **计算初始位置**：$H(36)=36\%13=10$。位置10已被23占用，**冲突**。
	        2.  **计算步长**：$H'(36)=(7\times36\%10)+1=(252\%10)+1=2+1=3$。
	        3.  **第1次探测**：$(10+1\times3)\%13=13\%13=0$。位置0由78占用，**冲突**。
	        4.  **第2次探测**：$(10+2\times3)\%13=(10+6)\%13=16\%13=3$。位置3由3占用，**冲突**。
	        5.  **第3次探测**：$(10+3\times3)\%13=(10+9)\%13=19\%13=6$。位置6由45占用，**冲突**。
	        6.  **第4次探测**：$(10+4\times3)\%13=(10+12)\%13=22\%13=9$。位置9为空，**放入**。
	        *   **查找次数**：初始1次 + 探测4次 = **5次**。
	3. 最终散列表

| 下标 | 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 | 12 |
| :--- | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: |
| **元素** | **78** | | **15** | **3** | | **57** | **45** | **20** | **31** | **36** | **23** | | **12** |

4. 计算平均查找长度（ASL成功）
	公式：$ASL_{success}=\frac{\sum(\text{每个元素的查找次数})}{\text{元素总个数}}$

	*   查找次数为1的元素有8个：12, 23, 45, 57, 20, 3, 78, 15。
	*   查找次数为3的元素有1个：31。
	*   查找次数为5的元素有1个：36。
	*   总次数 = $1\times8+3+5=16$。
	*   $ASL=\frac{16}{10}=1.6$。

- 衍生 
	-  处理冲突的方法：开放定址法 [[散列表处理冲突的方法]] 
	-  运算优先级
		- 题目图片中特别高亮了优先级。在计算 $H'(key)$ 时，例如 $7\times key\%10$，在编程语言和数学惯例中，乘法 $\times$ 和取模 $\%$ 优先级通常相同，结合性从左到右。即先算 $7\times key$，再对10取模
	- 改变冲突解决策略（最常见）   
		-  **线性探测法（Linear Probing）**：
		    *   公式：$H_i=(H(key)+i)\%m$。
		    *   即冲突了就往后看一个格子：$+1,+2,+3...$。
		    *   特点：容易产生“一次聚集”（堆积）。
		*   **平方探测法/二次探测法（Quadratic Probing）**：
		    *   公式：$H_i=(H(key)+d_i)\%m$，其中 $d_i=1^2, -1^2, 2^2, -2^2...$。
		    *   即冲突了就跳着找：$+1, -1, +4, -4...$。
	- 计算“查找失败”的平均查找长度（ASL失败） #查找失败的平均查找长度  
		- 这是比“查找成功”更难的考点。 
		*   **定义**：在散列表中搜索一个**不存在**的元素，平均需要探测多少次才能确定它不存在。
		*   **计算方法**：针对散列函数 $H(key)$ 可能映射到的每一个位置（0到12），计算从该位置开始查找，直到遇到“空位”为止所需的探测次数，然后求平均值。
		*   **公式**：$ASL_{unsuccess}=\frac{\sum_{j=0}^{m-1}(\text{从位置j查找到第一个空位的次数})}{m}$（注意分母通常是模数 $p$ 或表长 $m$，具体视教材定义而定，通常是除以$p$）。
	- 装填因子
		- 公式：$\alpha=\frac{\text{表中填入的记录数}}{\text{散列表的长度}}$。
		*   本题中 $\alpha=\frac{10}{13}\approx0.77$。
		*   考点：$\alpha$ 越大，发生冲突的可能性越大，查找效率越低。
	- #链地址法 
		- 如果题目不说“开放定址法”，而是说用“拉链法”或“链地址法”，则不需要计算探测步长，而是将冲突的元素挂在一个链表后面。这也是非常高频的考点。



![[Pasted image 20251215135412.png]]
[[Pasted image 20251119145409.png]]


- 答案之所以是 **正确 ($\sqrt{}$)**，其核心逻辑在于“不能保证”这四个字。这属于一道考查哈希表（散列表）基本原理与“除留余数法”特性的逻辑题   
#除留余数法 即散列函数为 $H(key)=key\%M$。 
1. 答案推导过程
	*   **基本前提**：散列表的长度（Table Length）设为 $L$。题目条件说 $M$ 是一个“不超过散列表长度 $L$ 的素数”（即 $M\le L$）。
	*   **哈希值的范围**：根据除留余数法的定义，计算出的哈希地址范围只能落在 $[0, M-1]$ 之间。
	*   **为什么无法保证均匀？**
	    *   **理想情况**：如果 $M$ 取值非常接近 $L$（例如 $L=13, M=13$），那么哈希地址会覆盖整个表长，分布相对均匀。
	    *   **极端情况（解析中的反例）**：题目只说了 $M$ 不超过 $L$，并没有说 $M$ 一定要接近 $L$。
	    *   假设表长 $L=100$，但我们选取的素数 $M=5$。
	    *   此时，所有关键字经过 $H(key)=key\%5$ 计算后，初始地址只会落在 $0,1,2,3,4$ 这 5 个位置上。
	    *   散列表剩下的 $5$ 到 $99$ 的位置，在初始哈希时永远不会被选中。只有在发生冲突（Collision）并采用开放定址法（如线性探测）时，数据才可能被迫“溢出”到后面的位置。
	    *   这种情况下，数据高度聚集在表头，绝大部分空间闲置，**分布极度不均匀**。
	
	**结论**：因为存在 $M$ 远小于表长 $L$ 导致分布极其糟糕的情况，所以题目说“不能保证其分布均匀”是正确的。
- 衍生 
	- 散列表的长度与模数的选取
		通常情况下，为了保证利用率，我们会让模数 $p$ 尽可能接近表长 $m$。
		*   如果 $p\ll m$（模数远小于表长），就会出现本题解析中的情况：大量空间被浪费，冲突率极高。
	- 除数 $p$ 的最佳选取 
		-  **问题**：如果散列表长为 $m$，除留余数法的模数 $p$ 应如何选择？
		*   **答案**：$p$ 应取不大于 $m$ 且最接近 $m$ 的**质数（素数）**。
		*   **反例**：如果 $p$ 选了合数（特别是 2 的幂），关键字的高位信息可能会丢失，导致冲突增加。
	- 装填因子的计算
		-   **定义**：装填因子 $\alpha$ 定义为散列表中已存元素数 $n$ 与表长 $m$ 的比值。
		    $\alpha=\frac{n}{m}$
		*   **考点**：$\alpha$ 越大，发生冲突的可能性越大，查找效率越低；$\alpha$ 越小，冲突越少，但空间浪费越多。
	- #计算散列表平均查找长度ASL    
		- 这是最常见的考法。给定一组 Key、表长 $m$、除数 $p$ 和冲突处理方法（线性探测或链地址法），让你手算：
			1.  **构建散列表**：画出每个 Key 的存储位置。
			2.  **计算 $ASL_{succ}$ (查找成功)**：$\frac{\sum\text{每个词查找次数}}{\text{关键词个数}}$。
			3.  **计算 $ASL_{unsucc}$ (查找失败)**：$\frac{\sum\text{每个哈希地址对应的探测次数}}{p}$（注意分母通常是模数 $p$，而非表长 $m$，具体需视教材定义而定）。



![[Pasted image 20251215135418.png]]
[[Pasted image 20251119150522.png]]

- 这道题目主要考察的是散列表（哈希表）处理冲突的方法中，开放定址法下的 #二次探测法 的一个特定性质  
1. “双平方探测”通常指的是**前后探测的二次探测法**。
	标准的二次探测增量序列是$d_i = 1^2, 2^2, 3^2, \dots, k^2$。
	而题目中的“双平方探测”指的是增量序列为：$d_i = 1^2, -1^2, 2^2, -2^2, \dots, k^2, -k^2$。
	即探测地址的公式为：
	$H_i = (H(key) \pm i^2) \pmod m$
2. 为什么需要 $m=4k+3$ 且为素数  
	1. 这是一个 #数论 中的结论。我们需要证明当表长$m$满足特定条件时，上述探测序列能遍历表中的所有位置（即 $0$ 到 $m-1$）
3. 推导逻辑如下
	1.  **二次剩余（Quadratic Residue）：**
	    如果在模$m$的意义下，存在整数$x$使得$x^2 \equiv a \pmod m$，则称$a$为模$m$的二次剩余。
	    对于素数$m$，在$1$到$m-1$中，恰好有$(m-1)/2$个二次剩余，和$(m-1)/2$个非二次剩余。
	2.  **正向探测 $H(key) + i^2$：**
	    $i^2 \pmod m$ 的取值恰好覆盖了模$m$的所有**二次剩余**（加上0）。
	3.  **反向探测 $H(key) - i^2$：**
	    这等价于加上 $-i^2$。关键在于，$-i^2$ 是否会和 $j^2$ 落入同一个集合？
	4.  **$m=4k+3$ 的性质：**
	    根据数论中的 #欧拉判别法 （Euler's criterion），对于奇素数$m$，$-1$是模$m$的二次剩余当且仅当$m \equiv 1 \pmod 4$。
	    反之，若$m \equiv 3 \pmod 4$（即题目中的$m=4k+3$），则$-1$是**非二次剩余**。
	    这意味着，对于$m=4k+3$：
	    *   $x^2$ 是二次剩余。
	    *   $-1$ 不是二次剩余。
	    *   所以 $-x^2 = (-1) \cdot x^2$ 必然**不是**二次剩余。
	5.  **结论：**
	    *   序列中的 $+i^2$ 覆盖了所有二次剩余的位置。
	    *   序列中的 $-i^2$ 覆盖了所有非二次剩余的位置。
	    *   加上初始位置（0），三者互不重叠且总数为 $1 + \frac{m-1}{2} + \frac{m-1}{2} = m$。
	    *   因此，当$m$是形如$4k+3$的素数时，双平方探测法一定能访问散列表中的全部$m$个位置。

- 衍生  
	- #单向二次探测的覆盖率   [[散列表处理冲突的方法]]
		- 如果只用单向的二次探测（增量为 $1^2, 2^2, \dots$），即使$m$是素数，它也**只能探测到散列表约一半的位置**（具体为 $(m+1)/2$ 个位置）。
		*   **考点：** 只有当散列表装填因子$\alpha < 0.5$且$m$为$4k+3$素数时，单向二次探测才能保证一定找到空位（如果存在）。
	-  #线性探测法 
		- 公式：$H_i = (H(key) + d_i) \pmod m$，其中$d_i = 1, 2, 3, \dots$。
		*   **考点：** 只要$m$和增量步长互质（线性探测步长通常为1，永远互质），线性探测就一定能遍历全表。但是它容易产生 堆积现象 ，降低查找效率。  
	* #散列表长度 $m$ 的选取   
		* **考点：** 为什么散列表的长度$m$通常取素数？
		    为了减少冲突。特别是在使用除留余数法（$H(key) = key \pmod p$）时，除数$p$（通常取$m$）最好是不大于$m$的素数，这能让哈希值分布更均匀  
	- #再散列法    [[再散列]]   
		- 公式：$H_i = (H(key) + i \cdot Hash_2(key)) \pmod m$。
		*   **考点：** 能够消除堆积现象。为了保证能遍历全表，要求步长$Hash_2(key)$必须与$m$互质。
- 总结公式
	对于题目中的定理，其数学表达为：
	若$m$为素数且$m \equiv 3 \pmod 4$，则集合
	$S = \{ (h + i^2) \pmod m \mid 0 \le i < m \} \cup \{ (h - i^2) \pmod m \mid 0 \le i < m \}$
	满足$|S| = m$。
	






![[Pasted image 20251215135424.png]]
[[Pasted image 20251119150529.png]]

- 推导主要依赖于散列表中 #平方探测法 的一个重要定理
1. **核心定理：**
	在使用**平方探测法**解决冲突的散列表中，如果表的大小$M$是一个素数（Prime Number），且装填因子（Load Factor）$\lambda\le0.5$，那么新的元素一定能够插入到表中（即一定能找到一个空位置）。  
2. 题目数据：
	*   散列表长度：$M=2017$（这是一个素数）。
	*   当前已存入元素：$N_{data}=1000$。
	*   冲突解决策略：单平方探测（只探测$+i^2$）。
3. 推导逻辑
	1. 为了保证平方探测法能够正常工作（即能够找到空位进行插入），根据定理，散列表中所有非空单元（包括“实际存有数据的单元”和“被懒惰删除标记的单元”）的总数不能超过表长的一半。[[懒惰删除]] 
	2.  计算允许的最大占用量：$Max_{total}=\lfloor M\times0.5\rfloor=\lfloor 2017\times0.5\rfloor=\lfloor 1008.5\rfloor=1008$。
	3.  题目问的是“此时最多有多少个懒惰删除的桶单元”。
	    *   设懒惰删除的元素个数为$N_{deleted}$。
	    *   实际存储元素个数为$N_{data}=1000$。
	    *   限制条件为：$N_{data}+N_{deleted}\le Max_{total}$。
	4.  代入数据：$1000+N_{deleted}\le1008$。
	5.  解得：$N_{deleted}\le8$。
4. 因此，最多有**8**个懒惰删除的桶单元，**答案选(A)**  
	1. 注：解析图片中提到的(B)选项的逻辑是基于$\lceil M/2\rceil=1009$，算出来是9，但这是不严谨的，标准的数据结构教材（如Weiss所著教材）通常强调$\lambda\le0.5$这一严格界限  

- 衍生 
	- #线性探测vs平方探测  
		-  **考点：** 对比两者的优缺点。
		*   **内容：** 线性探测（$d_i=i$）容易产生**一次聚集**，即冲突的元素会连成一片，导致查找效率下降。平方探测消除了由同义词引起的一次聚集，但如果两个不同的键哈希到了同一个初始位置，它们的探测路径将完全相同，这称为**二次聚集**。  
	*  #散列表大小 $M$ 的选择 
		*  **考点：** 为什么$M$要是素数？特别是$4k+3$形式的素数。
		*   **内容：**
		    *   若使用单向平方探测（只用$+i^2$），要求$M$是素数且$\lambda\le0.5$。
		    *   若使用双向平方探测（$d_i=\pm i^2$），如果$M$是形如$4k+3$的素数（例如11, 19, 23），定理证明探测序列可以遍历整个散列表的所有位置。
	-  #探测序列的计算  
		-  **考题形式：** 给定散列函数$H(k)=k\pmod{11}$，表长11，插入序列{12, 23, 34}，问34的位置。
		*   **计算：**
		    *   插入12：$12\%11=1$，位置1。
		    *   插入23：$23\%11=1$（冲突），探测$1+1^2=2$，位置2。
		    *   插入34：$34\%11=1$（冲突），探测$1+1^2=2$（冲突），探测$1+2^2=5$，位置5。
	- 再散列   
		-  **考点：** 什么时候扩容？
		*   **内容：** 当$\lambda$超过阈值（平方探测通常是0.5）时，需要创建一个大约是原来两倍大小的新素数表，将旧表中的**非删除**元素重新计算哈希值填入新表。注意， #懒惰删除 的元素在Rehashing过程中会被彻底丢弃，不再进入新表。  



![[Pasted image 20251215135436.png]]
[[Pasted image 20251119150536.png]]
- 题目问的是“封闭散列（即开放定址法）”相比“开放散列（即拉链法）”的优点    [[开放散列（拉链法）-封闭散列（开放定址法）]] 
1. 关于“不需要额外的空间”  
	1. #开放散列（拉链法）    
		1. 当发生冲突时，需要动态申请内存节点来存储数据，并通过指针连接。这就意味着，除了存储数据本身（Key/Value），由于每个节点都需要存储一个`next`指针，这就产生了额外的存储开销。如果是大规模数据，指针占用的空间也是可观的。此外，解析中提到的“多槽位”、“公共溢出区”也是类似原理，需要在主表之外开辟空间  
	2. #封闭散列（开放定址法）  
		2. 强调“就地解决”  
			1. 数据直接存放在散列表那个固定的数组里。如果位置$i$被占了，就根据规则（如线性探测）去检查$i+1$。它不需要指针，也不需要额外的溢出区，所有数据都在那一段连续的数组内存中  
	3. **推导结论**：省去了指针的空间开销，且不需要预留或动态分配表外空间  
2. 关于“更充分地利用缓存”  
	1. 开放散列（拉链法）    
		1. 使用的是链表。链表的节点在内存堆（Heap）中通常是随机分布的，物理地址不连续。CPU在读取`node->next`时，很难利用**局部性原理**进行预取，导致**缓存未命中（Cache Miss）**率较高。  
	2. 封闭散列（开放定址法） 
		1. 特别是其中的 #线性探测法 当冲突发生时，它是顺序检查数组的下一个位置。因为数组在内存中是连续存储的，当CPU读取位置$i$时，往往会将$i+1,i+2...$等后续内存块一同加载到CPU缓存（Cache Line）中。
	*   **推导结论**：查找冲突元素时，封闭散列拥有更好的**空间局部性**，能极大提高CPU缓存的命中率，从而提升速度。  

- 衍生    
	- 封闭散列（开放定址法）的缺点（高频考点）   
		-    **删除困难**：
		    在开放定址法中，不能简单地将物理空间清空。如果删除了位置$i$的元素，而位置$j$（$j>i$）的元素是当初因为$i$被占了才被挤到$j$的，那么查找$j$时，探测到$i$发现为空，就会误判元素不存在。
		    *   **解决方案**：必须使用**懒惰删除（Lazy Deletion）**，即不真删，而是打上一个“已删除（Deleted/Tombstone）”的标记。这会增加逻辑复杂度。
		*   **聚集/堆积现象（Clustering）**：
		    *   **一次聚集（Primary Clustering）**：线性探测特有的问题。如果一段连续的槽位被占用，任何散列到这个区域的新元素都会加长这个连续块，导致后续插入和查找越来越慢。
		*   **对装填因子敏感**：
		    封闭散列的装填因子$\alpha$绝对不能超过1。通常当$\alpha>0.5$或$\alpha>0.7$时，性能会急剧下降，必须进行**再散列（Rehashing）**（扩容并重新计算位置）。
	- 平均查找长度（ASL）计算  
		- 数据结构大题中最喜欢考的部分。你需要会算**成功查找**和**不成功查找**的平均比较次数。
		假设表长$m$，哈希函数$H(key)=key\%p$。
		*   #拉链法ASL ：
		    *   $ASL_{succ} \approx 1 + \alpha/2$
		    *   $ASL_{unsucc} \approx \alpha$
		*   #线性探测法ASL （公式较复杂，考试通常用手推）**：
		    *   $ASL_{succ} \approx \frac{1}{2}(1 + \frac{1}{1-\alpha})$
		    *   $ASL_{unsucc} \approx \frac{1}{2}(1 + \frac{1}{(1-\alpha)^2})$
		* **计算题小技巧**：
			计算“查找失败的ASL”时，不仅要看哈希值冲突，还要看在这个位置开始探测，直到遇到空位（NULL）为止需要比较多少次。这是很多考生容易丢分的地方。 

	


![[Pasted image 20251215135443.png]]
[[Pasted image 20251119150540.png]]

[[开放散列（拉链法）-封闭散列（开放定址法）]]  
1.  #缓存局部性原理 
	1. CPU在访问内存时，会把当前地址及其附近的数据块（Cache Line）一起读入高速缓存。如果程序顺序访问连续的内存地址（空间局部性），缓存命中率会很高，速度极快。
2. #封闭散列（开放定址法） 的特点
	1. 数据全部存放在一个连续的数组中。在解决冲突（如线性探测）时，往往是访问$index,index+1,index+2...$等相邻位置。这种连续的内存访问非常符合缓存的空间局部性，能**有效利用局部缓存**。
3.  #开放散列（拉链法） 的特点
	1. 解决冲突依赖于链表。链表的节点在内存堆中通常是随机分布的，物理地址不连续。遍历链表时，CPU需要在内存中“跳跃”访问，导致缓存未命中（Cache Miss）频繁，**不利于利用局部缓存**。
**结论：**
题目说法：“开放式散列...更可以有效利用局部缓存”。
实际情况：**封闭式散列**（连续数组）比开放式散列（链表）更能利用缓存。
因此，题目说法错误，答案为($\times$)。

- 衍生  
	- 具体的探测方法（开放定址法下）  
		- 考试常考具体的探测序列$d_i$计算：
			1.  **线性探测法（Linear Probing）**：
			    $d_i=1,2,3,...,m-1$。
			    *特点*：容易产生**一次聚集（Primary Clustering）**，即冲突的元素会连成一片，导致后续插入更慢。
			2.  **平方探测法（Quadratic Probing）**：
			    $d_i=1^2,-1^2,2^2,-2^2,...$。
			    *特点*：可以避免一次聚集，但可能出现**二次聚集**。
			3.  **再散列法（Double Hashing）**：
			    $H_i=(H(key)+i\cdot Hash_2(key))\pmod{m}$。
			    *特点*：使用两个散列函数，不易产生聚集。
	- 查找长度（ASL）计算  
		- 计算题的重灾区。需分别掌握**查找成功**和**查找失败**的平均查找长度。
			*   **查找成功 ASL**：$\frac{\sum\text{每个元素查找到所需的比较次数}}{n}$
			*   **查找失败 ASL**：$\frac{\sum\text{从每个散列地址出发确定失败所需的比较次数}}{m}$
			    *注意*：查找失败的分母通常是模数$m$（散列函数的值域大小），而不是元素个数$n$。
	- 删除操作的区别  
		- **问**：为什么开放定址法不能简单地将删除位置设为NULL？
		*   **答**：因为这会中断查找链。如果查找路径上的某个元素被物理删除了（置空），在查找其后发生冲突并探测到该位置后续的元素时，算法会误以为元素不存在。必须使用特定的**删除标记（Tombstone）**。
	- 性能比较  
		- 当装填因子$\alpha$较小时，开放定址法性能较好（无需指针开销，缓存快）。
		*   当装填因子$\alpha$接近或超过1时，拉链法性能优于开放定址法（开放定址法会陷入剧烈的冲突探测）。


![[Pasted image 20251215135453.png]]
[[Pasted image 20251119150556.png]]
[[Pasted image 20251119150604.png]]

1. 第一步：确定哈希函数  
	1. 题目给定哈希表长度 $M=13$，采用 #除留余数法 。通常 $H(key) = key \pmod P$，这里 $P$ 取 13。
		我们要插入的数据是 2021。
		$H(2021) = 2021 \pmod{13}$
		计算余数：$2021 = 13 \times 155 + 6$，所以初始哈希地址为 **6**。
2. 第二步：分析当前的哈希表状态  
	1. 根据题目第一张图的最终状态，我们可以看到以下位置已经被占用了：
		*   下标 **2**: 存放了 32
		*   下标 **3**: 存放了 84
		*   下标 **5**: 存放了 44
		*   下标 **6**: 存放了 18
		*   下标 **7**: 存放了 58
		*   下标 **9**: 存放了 35
		*   下标 **10**: 存放了 71
		被占用的集合为 $Occupied = \{2, 3, 5, 6, 7, 9, 10\}$。
3. 第三步：分析 #双向平方试探法 的探测序列  
	1. 冲突解决采用双向平方试探法，探测序列公式为：
		$H_i = (H(key) \pm i^2) \pmod M$  
	2. 其中 $i = 0, 1, 2, \dots, k$。
		对于 $key = 2021$，初始地址 $H(key) = 6$。
		我们需要找出所有可以通过该方法探测到的位置（也就是 $2021$ 能去哪）：
		1.  $i=0$: $6 \pmod{13} = 6$ （已被占用）
		2.  $i=1$:
		    *   $(6 + 1^2) \pmod{13} = 7$ （已被占用）
		    *   $(6 - 1^2) \pmod{13} = 5$ （已被占用）
		3.  $i=2$:
		    *   $(6 + 2^2) \pmod{13} = (6+4) \pmod{13} = 10$ （已被占用）
		    *   $(6 - 2^2) \pmod{13} = (6-4) \pmod{13} = 2$ （已被占用）
		4.  $i=3$:
		    *   $(6 + 3^2) \pmod{13} = (6+9) \pmod{13} = 15 \pmod{13} = 2$ （重复，已占用）
		    *   $(6 - 3^2) \pmod{13} = (6-9) \pmod{13} = -3 \pmod{13} = 10$ （重复，已占用）
		5.  $i=4$:
		    *   $(6 + 4^2) \pmod{13} = (6+16) \pmod{13} = 22 \pmod{13} = 9$ （已被占用）
		    *   $(6 - 4^2) \pmod{13} = (6-16) \pmod{13} = -10 \pmod{13} = 3$ （已被占用）
		6.  $i=5$:
		    *   $5^2 = 25 \equiv 12 \pmod{13}$。
		    *   $(6 + 12) \pmod{13} = 18 \pmod{13} = 5$ （重复）
		    *   $(6 - 12) \pmod{13} = -6 \pmod{13} = 7$ （重复）
		7.  $i=6$:
		    *   $6^2 = 36 \equiv 10 \pmod{13}$。
		    *   $(6 + 10) \pmod{13} = 16 \pmod{13} = 3$ （重复）
4. **结论：**
	无论 $i$ 取何值，探测序列只能产生以下这组索引：$\{6, 7, 5, 10, 2, 9, 3\}$。
	这 **7个位置** 刚好就是哈希表中已经被填满的那 7 个位置。因此，**2021 无法找到空位插入，插入失败**。

	- 这也正是第二张图（解析图）所表达的核心数学原理：$k^2 \pmod{13}$ 的取值只有 $\{0, 1, 3, 4, 9, 10, 12\}$ 这7种情况，导致对于基数 6 而言，探测只能覆盖一半的表空间  

- 衍生 
	- 线性试探法 vs 平方试探法  
		-    **题目**：如果采用线性试探法（$d_i = (d_0 + i) \pmod M$），2021 能否插入？
		*   **解析**：能。线性试探法只要表没满，最终一定能找到空位（因为它会遍历每一个位置）。但其代价是容易产生堆积，查找效率可能降低。
	- #装填因子 
		-   **公式**：$\alpha = \frac{\text{表中填入的记录数}}{\text{哈希表的长度}}$
		*   **题目**：计算插入 2021 之前的装填因子。
		*   **解析**：$\alpha = \frac{7}{13} \approx 0.54$。
		*   **意义**：$\alpha$ 越大，发生冲突的可能性越大。通常 $\alpha$ 超过某个阈值（如 0.75）时需要进行**再哈希 (Rehashing)**（扩容）。
	* 平方试探法能探测全表的条件  
		* **知识点**：如果哈希表长度 $M$ 是一个形如 $4k+3$ 的素数，且使用双向平方试探法（$\pm i^2$），则可以遍历整个哈希表。
		*   **分析本题**：本题 $M=13$，满足 $13 = 4 \times 3 + 1$，属于 $4k+1$ 型素数。因此， #双向平方试探法 在 $M=13$ 时**不能**覆盖全表，只能覆盖一半左右的位置（正如本题推导出的结论）。这是一个非常深入的数学考点。
	- 平均查找长度 (ASL)  
		-  **题目**：计算在当前表中查找成功的平均查找长度 $ASL_{success}$。
		*   **公式**：$ASL = \frac{\sum \text{每个元素查找成功的比较次数}}{\text{元素个数}}$
		*   **解析**：需要回溯每个元素的插入过程。
		    *   44 (1次)
		    *   35 (1次)
		    *   18 (1次冲突 -> 2次)
		    *   58 (1次冲突 -> 2次)
		    *   ...以此类推计算总和除以7。

![[Pasted image 20251215135507.png]]
[[Pasted image 20251119150611.png]]
- 题目考察的是哈希表 中采用**独立链又称拉链法）解决冲突时，查找成功的平均查找长度
1. 要计算查找成功的平均查找长度，我们需要计算查找所有个元素所需的比较次数总和，然后除以$n$
	1. 基本假设 
		1.  哈希函数将元素均匀地分布在$m$个槽位（Bucket）中。
		2.  新插入的元素通常放在链表的尾部（或者头部，对于平均查找长度的计算结果是一样的，只要假设任意两个元素发生碰撞时，A排在B前面的概率为$1/2$）。
2. 推导逻辑  
	1. 考察任意一个特定的元素$x$，计算查找它所需的期望比较次数$E[C_x]$。
		查找$x$的比较次数等于$x$在它所在链表中的位置（第几个节点）。
		$x$的位置 $= 1 +$ （在$x$之前插入且被哈希到同一个槽位的元素个数）。
	2. 对于任何除$x$以外的其他元素$y$（共有$n-1$个这样的$y$）：
		1.  $y$和$x$被哈希到同一个槽位的概率是$1/m$。
		2.  如果它们在同一个槽位，假设插入顺序随机，$y$排在$x$前面的概率是$1/2$。
	3. 因此，对于任意$y$，它排在$x$前面且在同一个槽位的概率是：
		$P(y \rightarrow x) = \frac{1}{m} \times \frac{1}{2} = \frac{1}{2m}$
		
		那么，查找特定元素$x$的期望比较次数为：
		$E[C_x] = 1 + \sum_{y \neq x} P(y \rightarrow x) = 1 + (n-1) \times \frac{1}{2m}$
		
		因为所有元素的期望都是一样的，所以整个哈希表查找成功的平均查找长度（期望值）为：
		$ASL_{success} = 1 + \frac{n-1}{2m}$
		
		**代入题目数据：**
		*   元素个数$n=2023$
		*   表长$m=119$
		
		$ASL_{success} = 1 + \frac{2023-1}{2 \times 119}$
		$= 1 + \frac{2022}{238}$
		$= 1 + \frac{1011}{119}$  (分子分母同时除以2)
		$= \frac{119 + 1011}{119}$
		$= \frac{1130}{119}$
- 这就是图片中黄色高亮部分修正后的答案  

- 衍生 
	- 独立链法查找不成功的平均查找长度  
		-   **问题：** 如果要查找一个不存在于表中的元素，期望需要比较多少次？
		*   **公式：** 在独立链法中，查找失败意味着要遍历完对应槽位的整个链表。平均链长为$\alpha = n/m$。
		    *   如果计算空指针的判断（通常计入比较），则约为$\alpha$或$\alpha + e^{-\alpha}$（视具体定义而定）。最常见的简单考法答案是$\alpha$。
	* #开放定址法   
		* 与 #独立链法 相对，所有元素都存放在哈希表数组中。
		*   #线性探测法 （Linear Probing）： 发生冲突时，顺序检查下一个位置。
		    *   成功查找ASL近似公式：$\frac{1}{2}(1 + \frac{1}{1-\alpha})$
		    *   失败查找ASL近似公式：$\frac{1}{2}(1 + \frac{1}{(1-\alpha)^2})$
		*   #二次探测法 （Quadratic Probing）：** 探测序列为$d_i = 1^2, -1^2, 2^2, -2^2...$
		*   **考点：** 开放定址法容易产生“ #堆积（开放定址法） （Clustering）”现象，且装填因子$\alpha$必须小于1
	- ASL的定义与计算细节 
		- 考试中常给出一组具体的数字和具体的哈希函数（如$H(key) = key \% 13$），要求手动模拟插入过程，画出哈希表，然后计算ASL。
		*   **计算方法：** $ASL = \frac{\sum \text{每个元素的查找次数}}{元素总数}$。 
	* 比较不同解决冲突方法的优劣  
		* **独立链法**适合：记录总数未知或经常变化，删除操作频繁的情况。
		*   **开放定址法**适合：记录总数固定，且$\alpha$较小的情况，不需要额外指针空间





![[Pasted image 20251119150618.png]]
[[Pasted image 20251119150624.png]]
- **核心逻辑：**
	在采用 #线性试探法 的 #散列表 中，“失败的删除”操作等同于“失败的查找”操作。也就是说，你要删除一个元素，首先得查找它，如果找不到（查找失败），就无法删除。
1. 在使用[[懒惰删除]]机制时   
	1. 被占用的槽位
		1. 查找时如果遇到，通过比较Key如果不相等，需要继续往下探测  
	2. 被懒惰删除标记的槽位（$L$） 
		1. 虽然逻辑上被删除了，但在物理探测链上它不能断（否则会找不到该位置之后插入的元素）。因此，查找过程中遇到标记为“Deleted”的槽位，必须**跳过并继续**往下探测。 
	3. 真正的空槽位（Null）  
		1. 只有遇到真正的空槽位（既没被占用，也没被标记删除），查找过程才会停止，从而断定元素不存在（删除失败）  
2. 数学建模
	*   散列表总规模：$M$
	*   导致探测“继续”的槽位数量：$N$（现有词条）+ $L$（懒惰删除标记）
	*   导致探测“停止”的槽位数量（真正的空位）：$M - N - L$
3. 可以将每一次探测看作一次随机抽样（这是计算机考研中常见的近似处理方法，即假设数据是均匀分布的）  
	1. 探测到任何一个位置，该位置能让查找“停止”（即它是真正的空位）的概率 $p$ 为：
		1. $p=\frac{\text{真正的空槽位数}}{\text{总槽位数}}=\frac{M-N-L}{M}$
	2. 我们想要计算的是“直到找到一个空位为止”的期望探测次数。这符合**几何分布**的期望模型。几何分布的期望 $E = 1/p$。  
		1. 计算结果
			1. $E=\frac{1}{p}=\frac{1}{\frac{M-N-L}{M}}=\frac{M}{M-N-L}$
	3. 因此，答案是 **(A)**  

- 衍生 
	- 成功的查找/删除的期望时间  
		- 题目问的是“失败”的情况。如果问“成功”的情况，公式会非常复杂，但在考试中通常考察定性分析：
			*   成功的查找只与插入时的历史状态有关，不受后续插入元素的影响，但受懒惰删除标记的影响（因为标记位占用了探测路径）。
		*   **考法**：可能会给具体的数字序列，让你模拟插入和删除过程，计算具体的平均查找长度（ASL）。
	- 懒惰删除后的插入逻辑  
		- **问题**：当查找一个键$K$准备插入时，如果探测路径上遇到了一个“Deleted”标记，应该怎么办？
		*   **答案**：通常记录遇到的**第一个**“Deleted”位置。如果后续确认$K$不存在（查找到Null），则将$K$插入到之前记录的那个“Deleted”位置，而不是当前的Null位置。这样可以回收空间，减少碎片。  
	* #再散列的时机  
		* **问题**：由于$L$的存在，即使$N$很小，散列表也可能被填满（$N+L$接近$M$）。
		*   **考法**：判断何时触发Resize（扩容或整理）。条件通常不再是 $N > M/2$，而是 $(N + L) > \text{Threshold}$。当“垃圾”标记过多时，即使不扩容，也需要重新整理（Rehash）以清除标记，恢复性能。
	* 与其他解决冲突方法的对比  
		* 链地址法
			* 链地址法通常不需要懒惰删除，因为它可以直接从链表中移除节点（虽然双向链表是$O(1)$删除，但单向链表需要遍历前驱）。
		- 二次试探
			- 同样需要懒惰删除，且会出现“二次聚集”现象，计算探测位置的公式变为 $(H(key) + i^2) \% M$。
- 总结
	这道题的本质是考察考生是否理解：在懒惰删除机制下，被标记删除的节点在查找过程中起到了“阻碍/桥梁”的作用，只有真正的空节点才是查找的终点。 答案 $O(\frac{M}{M-N-L})$ 正是基于空节点比例的倒数得出的  



![[Pasted image 20251215135518.png]]
[[Pasted image 20251119150631.png]]
- 这道题目的答案是基于散列表中处理冲突的一种特定方 法——  #双向平方试探法 的数学性质推导出来的   [[哈希冲突解决办法和堆积的概念]]
1. 核心结论：      #二次探测法 
	1. 若采用形如$d_i = \pm i^2$的 双平方探测策略（即增量序列为$1^2, -1^2, 2^2, -2^2, \dots$），为了让探测序列能够遍历散列表的所有位置（即保证能够找到空位并填入，或者查找时能遍历全表），**表长$M$必须是形如$4k+3$的素数
2. 推导过程
	1. 题目图片中的表格直观地展示了这个数学规律：
		1. **表格结构：**
		    *   第一行展示了探测增量：$\pm i^2$（如$0, 1, -1, 4, -4, \dots$）。
		    *   左侧列（$M$）展示了不同的表长：$5, 7, 11, 13$。
		    *   中间的数据是增量对$M$取模后的结果。
		2. 观察$M$的形式  
			1.  **$M=5$**：$5 = 4 \times 1 + 1$（即$4k+1$型）。表格中背景为粉色，表示效果不好。可以看到模运算后的结果有重复（例如$1$和$4$重复出现），无法覆盖所有位置。
			    *   **$M=13$**：$13 = 4 \times 3 + 1$（即$4k+1$型）。背景为粉色，同样无法覆盖全表。
			    *   **$M=7$**：$7 = 4 \times 1 + 3$（即$4k+3$型）。表格中背景为绿色，且数字$0, 1, 6, 4, 3, 2, 5$是$0$到$6$的全排列。这意味着探测序列能不重复地覆盖表中每一个位置。
			    *   **$M=11$**：$11 = 4 \times 2 + 3$（即$4k+3$型）。背景为绿色，同样能覆盖全表。
		3.  数学原理（简述）  
			1. 这是数论中的一个定理。如果$M$是素数且$M \equiv 3 \pmod 4$，那么$i^2 \pmod M$和$-(i^2) \pmod M$产生的余数集合是互补的（除了0），两者的并集恰好能覆盖$1$到$M-1$的所有非零余数。因此，配合$0$，整个序列能遍历整个散列表。
			2. 反之，如果$M \equiv 1 \pmod 4$，$-1$是模$M$的二次剩余，会导致$x^2$和$-x^2$映射到相同的余数集合，造成严重的“二次聚集”或无法遍历全表。
- 题目陈述说表长应取$4k+1$的素数，这与上述性质相悖，因此答案为**错误（$\times$）**，正确应为$4k+3$。  


- 衍生  
	- #线性探测vs平方探测  
		- **线性探测 (Linear Probing)：** $d_i = i$。
		    *   **优点：** 只要表没满，一定能找到空位。
		    *   **缺点：** 容易产生**堆积（Clustering）**或称“一次聚集”现象，即冲突的元素连成一片，降低查找效率。
		*   **二次探测 (Quadratic Probing)：**
		    *   **优点：** 能够缓解堆积现象（避免了一次聚集），但会出现“二次聚集”（即两个初始哈希值相同的键，它们的探测路径完全相同）。
		    *   **限制：** 对表长$M$有特殊要求（如本题所述）。
	- #装填因子  
		- 定义：$\alpha = \frac{n}{M}$，其中$n$是填入表中的元素个数，$M$是表长。
		*   **考法：** 题目可能会问，使用平方探测法时，为了保证探测能够成功，装填因子$\alpha$通常不宜过高。虽然双向平方探测在$M=4k+3$时能遍历全表，但在实际工程中（如单向探测），往往要求$\alpha < 0.5$以保证插入成功率 
	* 计算具体的探测地址  
		*  **题目示例：**
			设散列表长$M=11$，哈希函数$H(key)=key \pmod{11}$，采用双向平方探测法$d_i = \pm i^2$解决冲突。若已插入元素导致位置$2$被占用，现插入$key=13$。
			**解题思路：**
			1.  计算初始地址：$H(13) = 13 \pmod{11} = 2$。
			2.  发生冲突（位置$2$有人）。
			3.  第一次探测：$d_1 = 1^2 = 1$。新地址$=(2+1) \pmod{11} = 3$。若$3$空，填入；若满，继续。
			4.  第二次探测：$d_2 = -1^2 = -1$。新地址$=(2-1) \pmod{11} = 1$。
			5.  第三次探测：$d_3 = 2^2 = 4$。新地址$=(2+4) \pmod{11} = 6$。
			    ...以此类推。
	- 再散列法  
		- 这是另一种解决冲突的方法，即使用两个哈希函数。
			公式：$H_i = (H_1(key) + i \times H_2(key)) \pmod M$。
				考试中常考$H_2(key)$的选取原则（例如$H_2(key)$的值不能为$0$，且应与$M$互质）


![[Pasted image 20251215135524.png]]
[[Pasted image 20251119150635.png]]

- 这道题目的核心在于区分**单个节点的属性**与**整个数据结构的属性**。题目中的判断题之所以是错的，是因为它混淆了“单个词条（节点）的期望高度”与“整个跳转表（Skip List）的最大高度”  

1. 为什么单个节点的期望高度是 2？  
	1. 跳转表的构建过程中，决定一个节点高度（即它拥有多少层指针）通常是基于一个概率事件，类似于“抛硬币”   [[跳表]]  
2. 生成机制  
	1. 对于每一个节点，它至少在第1层（最底层）。是否上升到第2层，取决于抛硬币的结果（概率为$p$，通常取$p=1/2$）。如果成功上升到第2层，再抛一次硬币决定是否上升到第3层，以此类推。一旦失败，该节点的层数增长停止。
3. 数学推导  
	1. 服从 #几何分布 
		1. 假设抛硬币正面朝上的概率为$p=1/2$。
		    *   节点高度$H\ge1$的概率是$1$（必然存在）。
		    *   节点高度$H\ge2$的概率是$p$（抛一次正面）。
		    *   节点高度$H\ge3$的概率是$p^2$（连续两次正面）。
			*   ...
			*   节点高度$H\ge k$的概率是$p^{k-1}$。
	2. 单个节点的**期望高度**$E[H]$可以通过 #求和公式 计算： [[数列求和公式]] 
		1. $E[H]=\sum_{k=1}^{\infty}P(H\ge k)=1+p+p^2+p^3+\dots$
		    这是一个无穷 #等比数列求和 ，公比为$p$。当$p=1/2$时：
		    $E[H]=\frac{1}{1-p}=\frac{1}{1-0.5}=2$
4.  **结论：**
    无论跳转表中有多少个节点$n$，**单个节点**的期望高度是一个常数（当$p=1/2$时为2），属于$O(1)$，而不是题目中说的$\Theta(\log n)$。

- 衍生  
	- 整个跳转表的期望高度  #跳表的期望高度 
		- 这是题目中容易混淆的点。
		*   **问题：** 整个跳转表（$n$个节点）的最大高度是多少？
		*   **解析：** 我们需要找到$n$个独立随机变量的最大值。可以证明，当层数达到$k=\log_{1/p}n$时，期望剩余节点数约为1。
		*   **结论：** 整个跳转表的期望高度是$\Theta(\log n)$（具体来说是$\log_{1/p}n$）。这保证了查找路径的起点高度是对数级别的。
	* #跳表的空间复杂度   
		* **问题：** 跳转表占用的额外空间（指针数量）是多少？
			*   **解析：** 基于我们推导出的“单个节点期望高度为2”。
			    总指针数 = 节点数 $\times$ 单个节点期望高度 = $n\times2=2n$。
			*   **结论：** 空间复杂度是$O(n)$。如果题目问指针总数的期望值，答案是$2n$（当$p=1/2$时）。
	- 概率 $p$ 的影响   #跳表概率的影响   
		-   **问题：** 如果晋升概率$p$不等于$1/2$，比如$p=1/4$，会有什么影响？
		*   **解析：**
		    *   单个节点期望高度变为$\frac{1}{1-1/4}=4/3$（更省空间）。
		    *   查找的时间复杂度依然是$O(\log n)$，但底数变了，树变得更“扁”，可能导致横向查找步数增加。
		    *   这是一个**时间-空间权衡**（Space-Time Tradeoff）。$p$越小，空间越省，但层高越低，查找可能变慢；$p$越大，索引越密集，查找越快，但空间占用变大。
	- 最坏情况   #跳表最坏时间复杂度  
		- **问题：** 跳转表查找的最坏时间复杂度是多少？
			*   **解析：** 跳转表是概率型数据结构。在极端倒霉的情况下（例如所有节点都只有1层高，或者节点高度生成极其不均匀），它会退化成普通链表。
			*   **结论：** 最坏时间复杂度是$O(n)$。这与平衡二叉树（如AVL、红黑树）严格保证最坏$O(\log n)$不同。
	- 总结公式表

| 属性             | 期望值/复杂度          | 备注                    |
| :------------- | :--------------- | :-------------------- |
| **单个节点期望高度**   | $1/(1-p)$        | 当$p=0.5$时为$2$，即$O(1)$ |
| **整个表最大高度**    | $\Theta(\log n)$ | 也就是$O(\log_{1/p}n)$   |
| **查找/插入/删除时间** | $O(\log n)$      | 期望值                   |
| **空间复杂度**      | $O(n)$           | 指针总数约为$n/(1-p)$       |





![[Pasted image 20251119150644.png]]
[[Pasted image 20251119150650.png]]
- 这道题的答案是**正确 (√)**。题目的意思是：一个包含n个元素的跳表，在最坏情况下，其空间复杂度**可以**达到$Ω(n^2)$甚至更高  
	- 这是因为跳表的层高是由随机过程决定的，理论上没有上限。虽然在实际应用中，这种情况的发生概率极低，可以忽略不计，但这道题考查的是理论上的“极端情况”
1. [[跳表]]  
	1. 是一种基于**并联链表**的随机化数据结构，旨在实现与平衡二叉搜索树（如AVL树、红黑树）相媲美的性能。
		*   **结构**：它由多个层级的有序链表组成。
		    *   最底层（Level 0）是一个包含所有元素的普通有序链表。
		    *   每一层（Level $i$）都是其下一层（Level $i-1$）的“快速通道”，它只包含下一层中的部分节点。
		    *   一个节点如果在第 $i$ 层出现，那么它一定在所有低于 $i$ 的层中出现。
		*   **随机化**：当插入一个新节点时，我们会通过一个随机过程（通常是连续抛硬币）来决定这个节点应该“长”多高，即它最高会出现在第几层。
		    *   标准策略是：从第0层开始，抛一枚硬币。如果为正面，则将该节点也放入上一层（第1层），然后继续为第1层抛硬币；如果为反面，则停止。
		    *   这个过程决定了每个节点的“高度”。
	2. **优点**：实现比平衡树简单（无需复杂的旋转操作），且在期望情况下，查找、插入、删除的时间复杂度都是，空间复杂度为$O(n)$。 
2. 为何极端情况下空间会大于 $Ω(n^2)$？  
	1. 期望情况
		在标准的$p=1/2$概率下（即抛硬币），一个节点：
		*   出现在第0层的概率是1。
		*   出现在第1层的概率是$1/2$。
		*   出现在第k层的概率是$(1/2)^k$。
		1. 一个节点高度的期望值是 $\sum_{k=1}^{\infty} k \cdot (1/2)^k = 2$。因此，所有个节点的总指针数（代表空间）的期望值是乘以一个常数（大约是2），所以期望空间复杂度是$O(n)$ 
	2. 极端情况  
		1. 极端情况的发生，源于跳表高度的**随机性**。随机过程的结果有好有坏，而最坏情况就是随机结果“极其不理想”的时候  
	3. 构建一个导致空间复杂度爆炸的极端场景   
		1.  **单个节点的高度没有理论上限**：当我们插入一个节点时，我们通过抛硬币决定它的高度。理论上，我们**可能**连续抛出100次、1000次甚至$n^2$次正面。虽然概率极小（例如，连续$n^2$次正面的概率是$(1/2)^{n^2}$，趋近于0），但它在理论上是**可能发生**的。
		2.  **构造一个最坏场景**：假设我们运气差到极点。
		    *   我们插入第一个节点，随机过程让它连续抛出了$n$次正面，于是这个节点的高度达到了$n$。
		    *   我们插入第二个节点，它也达到了$n$的高度。
		    *   ...
		    *   我们插入全部$n$个节点，由于每次都运气极差，**所有$n$个节点的高度都达到了$n$**。
		3.  **计算空间复杂度**：
		    *   在这种情况下，跳表的总高度 $h=n$。
		    *   每个节点都在从第0层到第$n$层的所有层级中存在。
		    *   一个高度为$k$的节点，大约需要$k+1$个指针。如果所有$n$个节点的高度都是$n$，那么总的指针数量（即空间占用）大约是$n \times n = n^2$。
		    *   因此，总空间复杂度为$O(n^2)$。
		4.  **回到题目和解析**：
		    *   题目中的解析更进一步，它提到“极端情况$h$本身都可能是$Ω(n^2)$”。这也是理论上可能的。比如，第一个插入的节点，我们连续抛出了$n^2$次正面，使其高度$h=n^2$。
		    *   如果此时总共有$n$个节点，而最高层的高度为$h=n^2$，那么总空间至少是$Ω(nh)$，即$Ω(n \cdot n^2) = Ω(n^3)$。
		    *   既然空间可以达到$Ω(n^3)$，那么它自然也**大于$Ω(n^2)$**。所以原题的说法是正确的。
		
	**总结**：虽然跳表在平均情况下表现优异，但其性能保证是**概率性**的，而非确定性的。正是这种随机性，导致了理论上存在性能极差的极端情况，尽管在现实中几乎不可能遇到。
- 衍生 
	- 平均时间/空间复杂度  
		- **问题**：请证明跳表的期望空间复杂度为$O(n)$，期望查找时间复杂度为$O(\log n)$。
	    *   **考点**：这需要利用概率论和级数求和的知识。空间复杂度的证明相对简单，时间复杂度的证明则需要分析查找路径的期望长度。  
	* 与平衡二叉搜索树的对比   #跳表对比红黑树和B➕树  
		* **问题**：跳表和红黑树都是高效的查找数据结构，请比较它们的优缺点。
	    *   **考点**：
	        *   **性能保证**：红黑树提供**确定性**的$O(\log n)$时间复杂度，而跳表是**概率性**的。
	        *   **实现复杂度**：跳表的插入删除操作比红黑树的旋转、变色等操作要简单直观。
	        *   **并发性能**：跳表的局部修改特性使其在并发环境中更容易实现高效的无锁操作，而树的旋转操作通常需要锁定较大范围的节点。
	        *   **空间占用**：跳表的平均空间占用通常比平衡树要大（因为有很多“上层”指针）。
	- 概率的影响  #跳表概率的影响  
		- **问题**：在跳表中，如果我们将节点“晋升”到上一层的概率从$1/2$改为$1/4$，会对时间和空间产生什么影响？
	    *   **考点**：这是一个 trade-off（权衡）问题。
	        *   **空间**：概率$p$变小，节点的平均高度会降低（期望高度为$1/(1-p)$），所以平均空间占用会**减少**。
	        *   **时间**：层数变少，每层的节点也更稀疏，导致“快速通道”效果减弱，查找时需要向下移动的次数增多，平均查找时间会**增加**。

![[Pasted image 20251215135539.png]]
[[Pasted image 20251119150655.png]]

- 跳表（Skip List）是一种基于**概率**的数据结构。它的核心思想是通过抛硬币（或随机数生成器）来决定一个节点是否需要提升到上一层索引中
假设我们有$n$个元素：
1.  **底层（第0层）**包含所有的$n$个元素。
2.  对于每一层的一个节点，我们以概率$p$（通常$p=1/2$）将其提升到上一层。这意味着上一层的节点数期望是当前层的一半。
    *   第1层期望有$n\times p$个节点。
    *   第2层期望有$n\times p^2$个节点。
    *   ...
    *   第$h$层期望有$n\times p^h$个节点。
	
	跳表的高度$h$定义为当顶层节点的期望数量为$1$时的层数。根据上述规律，我们可以列出方程：
		$n\times p^h=1$
		即：
		$p^h=\frac{1}{n}$
	两边取对数（通常以$1/p$为底，若$p=1/2$则以2为底）：
		$h=\log_{1/p}n$
	在算法复杂度分析中，对数的底数不影响大O记号的级别，因此高度的期望值为：
		$O(\log n)$
	这就是为什么答案选**(B)**的原因。
[[跳表]] 

- 衍生 
	-  跳表与红黑树/B+树的对比 #跳表对比红黑树和B➕树 
		- **与红黑树对比**：
		    *   **实现难度**：跳表实现比红黑树简单得多（不需要处理复杂的旋转）。
		    *   **区间查找**：跳表支持区间查找（Range Query）非常方便，只需要找到起点然后向后遍历即可；而红黑树需要复杂的逻辑。 
		    *   **并发性能**：在并发编程中，跳表只需要局部加锁（更新节点附近的指针），而红黑树可能涉及整树的再平衡，跳表更适合高并发场景。
		*   **与B+树对比**：
		    *   B+树更适合磁盘存储（减少IO次数），因为B+树节点大、层高低；跳表更适合内存存储（如Redis）。
	- 为什么Redis的有序集合（ZSet）使用跳表   #跳表（Redis） 
		- 这是一个非常经典的工业界应用问题。
			*   **原因1**：性能与红黑树相当，都是$O(\log n)$。
			*   **原因2**：实现简单，代码易于维护。
			*   **原因3**：**范围查找（Range Scan）**操作非常频繁（如排行榜），跳表在范围查找上效率极高且实现直观。
	- 概率 $p$ 的选择 #跳表概率的影响     
		题目中默认$p=1/2$，但在实际工程中（如Redis），$p$可能取$1/4$。
		*   如果$p=1/2$，平均每个节点2个指针，树高约$\log_2n$。
		*   如果$p=1/4$，平均每个节点1.33个指针，树高约$\log_4n$。
		*   **考点**：$p$越小，空间越省，但层数变低会导致查找时的横向跨度变大，查找效率略微降低。这是**空间与时间**的权衡。
	* 最坏情况分析
		虽然期望是$O(\log n)$，但如果运气极差（比如随机函数一直返回“不提升”或“一直提升”），会发生什么？
		*   **最坏时间复杂度**：退化成单链表，为$O(n)$。
			*   注意：这是概率极小的事件，在工程上通常忽略不计。
* 总结
	看到“跳表”和“高度”或“复杂度”，直接反应就是**对数级**$O(\log n)$。它是一种用概率来换取实现简单性的优美数据结构。


[[跳表]]



![[Pasted image 20251119150702.png]]







