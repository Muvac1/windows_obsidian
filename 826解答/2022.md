![[826/826试题.pdf#page=26&rect=21,705,442,742|826试题, p.26]]
T ， #渐近分析 
- 设 $f(n) = [logn]!$ 和 $g(n) = n^{loglogn}$  忽略取整符号 `[]` (floor function)，因为它不影响最终的增长率级别。所以我们分析 $f(n) = (logn)!$    [[斯特林公式]] 
1. 需要用到 #斯特林公式 来处理阶乘的对数。斯特林公式的一个常用形式是：
	$log(k!) = \Theta(klogk)$    [[时间复杂度分析]]
	在这里，我们令 $k = logn$。将其代入斯特林公式中，得到：
	$log(f(n)) = log((logn)!) = \Theta(logn \cdot log(logn))$
2. 对右边的函数 $g(n) = n^{loglogn}$ 取对数  
	1. 令 $k = logn$。将其代入斯特林公式中，得到：
		$log(f(n)) = log((logn)!) = \Theta(logn \cdot log(logn))$ 
	2. 对右边的函数 $g(n) = n^{loglogn}$ 取对数  
		1. 对数运算法则 $log(a^b) = b \cdot log(a)$，我们有：
			$log(g(n)) = log(n^{loglogn}) = (loglogn) \cdot logn$ 
3. 比较两者对数的结果：
	我们发现：
	*   $log(f(n)) = \Theta(logn \cdot loglogn)$
	*   $log(g(n)) = logn \cdot loglogn$
	两者取对数后的增长率是相同的，即 $log(f(n))$ 和 $log(g(n))$ 是同阶的 
	1. 因为 $log(f(n)) = \Theta(log(g(n)))$，这意味着存在正常数 $c_1, c_2, n_0$，当 $n > n_0$ 时，满足 $c_1 \cdot log(g(n)) \le log(f(n)) \le c_2 \cdot log(g(n))$ 
	2. 对这个不等式两边取指数，可以得到 $(g(n))^{c_1} \le f(n) \le (g(n))^{c_2}$
		1.  $f(n)$ 的增长率不高于 $(g(n))^{c_2}$
		2. 可以得出 $f(n) = \Theta(g(n))$  
	3. 原命题 $[logn]! = O(n^{loglogn})$ 是**正确**的  
4.   $f(n) = O(g(n))$ 表示函数 $f(n)$ 的增长率的上界是 $g(n)$。严格定义是：存在正常数 $c$ 和 $n_0$，使得对所有 $n \ge n_0$，都有 $0 \le f(n) \le c \cdot g(n)$   #大O表示法 
- 衍生  
	- [[函数增长率排序]] 
		- 如 $n!$, $2^n$, $n^{logn}$, $(logn)^n$, $n^2$, $nlogn$ 等，要求按增长率从低到高排序。这类问题需要熟练运用取对数等技巧进行比较。
	    *   例如，比较 $n^{logn}$ 和 $(logn)^n$。
	        *   取对数后得到 $logn \cdot logn = (logn)^2$
	        *   取对数后得到 $n \cdot log(logn)$
	        *   因为 $n$ 的增长远快于 $(logn)^2$，所以 $(logn)^n$ 的增长率远高于 $n^{logn}$
	- 证明或证伪其他复杂度关系  
		- 判断 $2^n = O(n!)$ 是否成立？ (答案：正确，可以用斯特林公式证明)
	    *   判断 $n! = O(2^n)$ 是否成立？ (答案：错误)
	    *   判断 $log(n!) = \Theta(n^2)$ 是否成立？ (答案：错误，因为 $log(n!) = \Theta(nlogn)$)
	- #递归算法的复杂度分析  
		- 一些复杂的递归式，如 $T(n) = n \cdot T(n-1)$，其解为 $n!$，这时就需要用到阶乘的复杂度来表示



![[Pasted image 20251112045152.png]]
#归并排序  #归并算法  
#二路归并  是归并排序 (Merge Sort) 算法中的核心操作。它的任务是将两个**已经有序**的子序列合并成一个单一的、完全有序的序列。     [[二路归并]]    [[排序算法]]  
1.    #有序向量 ：在数据结构中，这通常指 #有序数组   
	1.  **分析**：对于两个有序数组，我们可以完全按照上面描述的二路归并思想来实现。我们需要一个额外的辅助数组来存放合并后的结果。通过两个指针分别遍历两个输入数组，比较元素并放入辅助数组。这个过程需要遍历所有元素一次，因此时间复杂度为 $O(n)$  
2. #有序列表 ：这通常指 #有序链表
	1.  **分析**：对于两个有序链表，我们同样可以使用双指针法进行合并。一个指针指向链表A的当前节点，另一个指针指向链表B的当前节点。我们比较两个节点的值，将值较小的节点链接到新的合并链表的尾部，然后移动相应链表的指针。这个过程同样是遍历所有节点一次。
    *   **优点**：与数组不同，链表的归并操作**不需要额外空间**。我们只需要修改节点的 `next` 指针，将它们重新组织成一个新的链表即可。
4. 因为有序列表（链表）的二路归并算法**也**能在线性时间内完成，所以原命题“有序向量的二路归并算法能在线性时间内完成, 有序列表则不能”是**错误**的   

- 衍生 
	- #归并排序空间复杂度 
        *   对于**数组**，归并排序需要一个大小为 $n$ 的辅助数组，所以空间复杂度是 $O(n)$。
        *   对于**链表**，由于归并操作可以原地（通过修改指针）完成，所以空间复杂度主要来自递归调用的栈深度，为 $O(\log n)$。这也是为什么**归并排序特别适合用于链表排序**的原因。
	- 适用场景 
		-    **快速排序**：通常在内部排序（数据能全部载入内存）中，由于其平均性能和空间效率，是首选算法之一。
        *   **归并排序**：非常适合**链表排序**和**外部排序**（数据量巨大，无法一次性装入内存）。当需要稳定的排序时，归并排序也是一个很好的选择。


![[Pasted image 20251112045229.png]]
了解两个核心概念：**伸展树** 和用于分析其效率的 [[摊还分析与势能法]]   
[[伸展树]]  
1. 根据势能函数的定义来推导其最大值  
	1.   设树 $T$ 中共有 $n$ 个节点。
	2.  对于树中的任意一个节点 $x$，其子树大小 $s(x)$ 的取值范围是 $[1, n]$。因为一个节点至少包含它自己（大小为1），最多包含整棵树的所有节点（大小为 $n$）。
	3.  因此，对于任意节点 $x$，其秩 $r(x) = \log_2(s(x))$ 的最大值发生在 $s(x)=n$ 时，即 $r(x) \le \log_2(n)$。
	4.  树的总势能是所有节点秩的和：
	    $\Phi(T) = \sum_{x \in T} \log_2(s(x))$
	5.  我们将每个 $\log_2(s(x))$ 都替换为其可能的最大值 $\log_2(n)$，可以得到一个上界：
	    $\Phi(T) = \sum_{x \in T} \log_2(s(x)) \le \sum_{x \in T} \log_2(n)$
	6.  因为树中总共有 $n$ 个节点，所以这个求和共有 $n$ 项：
	    $\sum_{x \in T} \log_2(n) = n \cdot \log_2(n)$
	7.  因此，我们得出结论：
	    $\Phi(T) \le n \log_2(n)$
 2. 这个不等式表明，伸展树的总势能 $\Phi(T)$ 的上界是 $n \log_2(n)$。用大O记法表示，就是 $\Phi(T) = O(n\log n)$。  
- 衍生 
	- 伸展树操作的   #摊还复杂度   
		- **考点**：证明伸展树的 `splay` 操作（以及查找、插入、删除等）的摊还时间复杂度为 $O(\log n)$。  
		- **简介**：这是使用 #势能法的最终目的 。通过分析 `zig`, `zig-zig`, `zig-zag` 三种旋转操作导致的势能变化，可 以证明 `splay` 操作的摊还代价被一个 $O(\log n)$ 的项所约束（具体为 Access Lemma 的证明：$AmortizedTime(splay(x)) \le 3(r(Root_{final}) - r(x)) + 1 = O(\log n)$）。
	- 最坏情况与摊还情况的对比  
		- **考点**：请说明伸展树单次操作的最坏时间复杂度和摊还时间复杂度的区别。  
			- 单次操作的最坏复杂度是 $O(n)$，这发生在树极度不平衡时。而摊还复杂度 $O(\log n)$ 意味着，一个包含 $m$ 次操作的序列，其总时间代价是 $O(m \log n)$，平均下来单次操作是 $O(\log n)$。高代价的操作必然伴随着势能的大幅下降，为未来的高代价操作“储蓄”了时间  
	- 伸展树势能的最小值  
		-   **考点**：伸展树的总势能何时达到最小？最小值大约是多少？  
			-  **简介**：当树的形态为一棵**完全二叉树**（最平衡的形态）时，总势能最小。此时，大部分节点的子树规模 $s(x)$ 都比较小，因此 $\log_2(s(x))$ 的值也较小。其总势能的下界是 $\Omega(n)$。
			    *   相对地，当树退化成一条**链**时，总势能最大，接近 $O(n \log n)$。因为此时各节点的子树大小为 $1, 2, 3, ..., n$，总势能为 $\sum_{i=1}^n \log_2(i) = \log_2(n!) \approx O(n \log n)$

![[Pasted image 20251112045237.png]]
 [[左式堆]] 
[[零路径长]]  
- 题目中的说法是“左式堆的兄弟子堆满足 $L.height \ge R.height$”，即左子树的高度总是大于等于右子树的高度。这个说法混淆了**高度 (height)** 和 **零路径长 (npl)**  
	*   **高度 (Height):** 从一个节点到其最远叶子节点的最长路径长度。
	*   **零路径长 (npl):** 从一个节点到最近的“缺失”子节点的最短路径长度。
1. 左倾性保证了树的形态向左边“倾斜”，使得树的右侧路径非常短。这个性质的目的是为了让合并（merge）操作能够高效地在 $O(\log N)$ 时间内完成，因为合并操作总是沿着两个堆的右侧路径进行的。  
下面是一个合法的小根堆左式堆，但它不满足 $height(L) \ge height(R)$。
[[树的高度和树的深度的区别]]
考虑根节点 P，它的左孩子是 L，右孩子是 R。
*   令 L 是一个单独的叶子节点。
    *   $npl(L) = 0$
    *   $height(L) = 0$
*   令 R 是一个有两个节点的子树，R 有一个左孩子 R_child，但没有右孩子。
    *   $npl(R) = 1 + \min(npl(R_{\text{child}}), npl(\text{null})) = 1 + \min(0, -1) = 0$
    *   $height(R) = 1 + height(R_{\text{child}}) = 1 + 0 = 1$
	对于根节点 P 来说：
	*   $npl(L) = 0$
	*   $npl(R) = 0$
	*   由于 $npl(L) \ge npl(R)$ (即 $0 \ge 0$)，这满足左式堆的**左倾性**。
	*   但是，$height(L) = 0$，$height(R) = 1$。
	*   这里 $height(L) < height(R)$，与题目中的 $L.height \ge R.height$ 相悖。

- 衍生 
	- 核心操作的实现  
		- 合并
			- 这是左式堆最核心的操作，也是最高效的操作。插入和删除都基于合并。考试中可能会要求你手写合并两个左式堆的代码或描述其递归过程。合并操作的时间复杂度为 $O(\log N)$ 
		- 插入  
			- 将一个新元素看作一个单节点的左式堆，然后与原堆进行合并
		- 删除最小/大值
			- 删除根节点，然后将其左右两个子堆进行合并。
	- #左式堆的性质 与证明  
	- 与其他数据结构的对比：  
		- 与 #二叉堆  (Binary Heap) 对比  
			-  二叉堆的插入和删除操作效率很高 ($O(\log N)$)，但合并两个堆的效率很低 ($O(N)$)。左式堆的优势在于高效的合并操作。  
		- 与 #斜堆  (Skew Heap) 对比 
			-  斜堆是左式堆的一种“自适应”版本，它不存储和计算 npl，而是在合并时简单地、无条件地交换左右子堆。其操作具有均摊 $O(\log N)$ 的时间复杂度，实现上更简单  
		- 与 #二项堆  (Binomial Heap) /  #斐波那契堆  (Fibonacci Heap) 对比 
			- 这些都是更高级的可合并堆，在某些操作上（如 decrease-key）具有更好的理论复杂度，但实现也更复杂  



![[Pasted image 20251112045249.png]]
- #无向连通图  #关节点  [[关节点]]  
- #DFS生成树   在 DFS 过程中，所有经过的边（即从一个未访问顶点到一个新访问顶点的边）构成了一棵树，称为 DFS 生成树。这棵树的根就是起始顶点 `s`。 
1. 前提条件分析： 
	1. 我们从顶点 `s` 开始进行 DFS。`s` 是 DFS 生成树的根节点。
	    “DFS 树中 `s` 的度数为 1” 这句话意味着，在 DFS 树中，根节点 `s` **只有一个孩子节点**。我们称这个孩子节点为 `c`
2. 推理过程 
	1.   `s` 只有一个孩子 `c`，这说明 DFS 算法从 `s` 出发，访问了它的一个邻居 `c`，然后从 `c` 开始，遍历了图中所有其他可以到达的顶点，最后才回溯到 `s`。当从 `c` 的递归调用返回到 `s` 时，`s` 的所有其他邻居都已经被访问过了（否则它们会成为 `s` 的另一个孩子，导致 `s` 的度数大于 1）。
	2. 这意味着，除了 `s` 之外，图中的所有其他顶点都属于以 `c` 为根的子树。换句话说，**所有其他顶点在不经过 `s` 的情况下，彼此之间是连通的**（它们至少可以通过 DFS 树中的路径相连） 
	3. 判断 `s` 是否是关节点。根据定义，我们需要看移除 `s` 后，图是否会变得不连通 
	4. 当我们 移除 `s` 后，剩下的图由所有其他顶点组成。正如我们刚才分析的，所有这些顶点都属于以 `c` 为根的子树，它们本身构成了一个连通的子图。    
	5.    因此，移除 `s` 只是将 `s` 本身从图中拿掉，剩下的部分仍然是一个连通图。图的连通分量数量没有增加。
	    *   根据关节点的定义，`s` **不是** 关节点。
	**结论：** 该命题是正确的。

- 衍生 
	- 如何判断一个顶点是否为关节点  
		- 通常使用 **Tarjan 算法** 或类似的基于 DFS 的方法来解决。算法的核心是计算两个数组
	-  #桥的判断 
		- **定义：** 移除后会使图的连通分量数量增加的边。
		*   **判断：** 在 DFS 树中，一条边 `(u, v)`（其中 `u` 是 `v` 的父亲）是桥，当且仅当从 `v` 及其子树出发，不存在任何返祖边可以到达 `u` 或 `u` 的祖先。
		*   **公式：** $low[v] > dfn[u]$。
	 - #双连通分量 
		 - #点双连通分量 (v-BCC): 一个极大的子图，其中任意两点之间都至少有两条点不重复的路径。关节点是连接不同点双连通分量的“枢纽”。
		*  #边双连通分量 (e-BCC): 一个极大的子图，其中任意两点之间都至少有两条边不重复的路径。桥是连接不同边双连通分量的“唯一通道”。
			

![[Pasted image 20251112045302.png]]
- 这句话的意思是：在对一个无向图进行广度优先搜索（BFS）时，任何时刻，存在于辅助队列中的任意两个节点，它们各自到起始节点的距离（最短路径长度）之差的绝对值不会超过1。  
1. [[广度优先搜索（BFS）和深度优先搜索（DFS）]]  
	1. BFS的本质是“逐层”或“波浪式”扩展。它首先访问起始节点，然后访问所有与起始节点直接相邻的节点，再然后是与这些节点相邻的、尚未访问过的节点，以此类推。 
2. #BFS与距离的关系
	1.  在一个无向无权图中，BFS找到的从起始节点`s`到任何其他节点`v`的路径，都是**最短路径**。路径的距离就是边的数量。因此，BFS是按距离逐层遍历的：
	    *   第0层：起始节点`s`，距离为0。
	    *   第1层：所有与`s`直接相连的节点，距离为1。
	    *   第2层：所有与第1层节点相连、但未被访问过的节点，距离为2。
	    *   第`k`层：所有与第`k-1`层节点相连、但未被访问过的节点，距离为`k`。
3. 分析 #辅助队列 中的节点  
	1.  BFS使用一个队列（Queue）来实现这种逐层遍历，队列的特性是“先进先出”（FIFO）  
	2. **初始状态：** 队列中只有起始节点`s`，距离为0。此时结论成立。
	    *   **第一步：** `s`出队，`s`的所有邻居（第1层节点，距离都为1）入队。此时队列中所有节点的距离都是1，距离差为0，结论成立。
	    * **关键步骤：** 当我们开始处理第`k`层的节点时，我们会将它们逐个出队。每当一个第`k`层的节点`u`出队时，我们会把它所有未被访问过的邻居`v`（这些邻居就构成了第`k+1`层）加入队列。  
	    * 过程中，队列中会同时存在**尚未处理完的第`k`层节点**和**刚刚入队的第`k+1`层节点**  
	    * 在任何一个时间点，BFS的辅助队列里最多只包含来自两个相邻层次的节点：第`k`层和第`k+1`层  
	    * 设队列中有任意两个节点`u`和`v`，它们的距离`d(s, u)`和`d(s, v)`只可能是`k`或`k+1`。
    *   那么它们距离的差的绝对值`|d(s, u) - d(s, v)|`只可能是以下三种情况：
        *   `|k - k| = 0`
        *   `|(k+1) - (k+1)| = 0`
        *   `|(k+1) - k| = 1`
    *   在所有情况下，这个差值都**不大于1**
- 衍生 
	- 对DFS提出类似问题  
		-  **问题：** “对于无向图,从任意顶点出发,进行DFS,DFS辅助栈中的节点与起始节点距离相差均不大于1。”
	    *   **答案：** 这个说法是 **错误** 的。因为DFS会深入一条路径，栈中可能同时存在离起始点很近的节点（如起始点的直接邻居）和离起始点非常远的节点（在一条长路径的末端）。例如，对于路径`s-a-b-c-d`，当访问到`d`时，栈中可能还保留着`s`的另一个邻居`e`，此时`d`的距离是4，`e`的距离是1，距离差为3，远大于1。
	* #加权图的最短路径  
		* 题目强调的是无向图（可以看作是所有边权重为1的加权图）。如果图是带权重的（边的长度不同），BFS就不再适用。
	    *   **考点：** 会引出Dijkstra算法（用于无负权边）或Bellman-Ford算法（可处理负权边）。  


![[Pasted image 20251112045307.png]]
- 这道题的核心在于理解 AVL 树在**删除**操作后进行 #平衡调整（旋转）对其高度的影响 ，这与**插入**操作后的调整有所不同  
1. AVL插入操作的高度变化    
	1. 当向 AVL 树插入一个节点导致节点 `g` 失衡时，无论是进行单旋转还是双旋转，调整的目的是恢复平衡。一个关键特性是，**插入操作后的平衡调整，会使调整后的子树（新根）高度恢复到插入前 `g` 所在子树的高度**。因此，调整后高度不会向上传播变化，整个插入操作最多只需要一次旋转  
2. AVL 删除操作的高度变化   
	1. 当从 AVL 树删除一个节点时，可能会导致其父节点或祖先节点的某个子树高度减 1。这个高度的降低会向上传播，直到找到第一个不平衡的节点 `g`。
    此时，节点 `g` 的平衡因子变成了 +2 或 -2。我们对以 `g` 为根的子树进行旋转调整。与插入操作不同，**删除操作后的平衡调整，并不能保证恢复 `g` 的原始高度**。事实上，调整后的子树高度**一定会比删除前的原始高度减 1**。

	2. 为什么高度会降低？   
		1. 删除操作是实实在在地移除了一个节点，导致路径变短。旋转操作只是通过重新组织节点来满足平衡因子的要求（-1, 0, 1），但它无法“凭空”创造出被删除的节点所贡献的高度。因此，平衡后的子树高度必然会降低。  
```
      ...
       |
       50 (g, 高度=4)
      /  \
    30(p, 高度=3)  70(q, 高度=2)
   /   \          /
 20(高度=2) 40(高度=1)  60(高度=1)
 /
10(高度=1)
```
现在，我们从较矮的右子树中删除一个节点，比如 `60`。这会导致 `q` (70) 的高度从 2 降低到 1。

```
      ...
       |
       50 (g, 高度=4 -> 3)
      /  \
    30(p, 高度=3)   70(q, 高度=1)
   /   \
 20(高度=2) 40(高度=1)
 /
10(高度=1)
```
**旋转后的新结构：**

```
      ...
       |
       30 (新的根)
      /  \
    20     50 (原来的 g)
   /      /  \
 10     40    70
```

4. 计算新高度：见证高度的降低
	1.  叶子节点 `10`, `40`, `70` 的高度都是 **1**。
	2.  节点 `20` 的高度 = `h(10) + 1` = **2**。
	3.  节点 `50` 的高度 = `max(h(40), h(70)) + 1` = `max(1, 1) + 1` = **2**。
	4.  新的根节点 `30` 的高度 = `max(h(20), h(50)) + 1` = `max(2, 2) + 1` = **3**。
	**子树的高度确实从 4 降低到了 3！** 
- 衍生 
	- [[平衡因子]]  
	- 失衡与旋转  [[平衡二叉树的旋转]]  



![[Pasted image 20251112045315.png]]
#红黑树rb-tree  
1. **翻译：**
	如果一次红黑树的删除操作，在修复双黑（double-black）节点的过程中，执行了 Ω(logn) 次重染色操作，那么这个修复过程必然包含至少一次旋转。  
2. 深入了解红黑树删除操作后[[双黑修正]]
	1. [[红黑树]]   
	2. **结论推导：**
	题目中的前提是“进行了 $Ω(\log n)$ 次重染色”。这里的 $n$ 是树中节点的数量，而红黑树的高度是 $O(\log n)$。因此，$Ω(\log n)$ 次重染色意味着修复过程沿着树的路径从下往上，经过了与树高成正比的步数。
	
	*   观察上述修复情况，唯一能让双黑问题持续向上传播（从而产生多次重染色）的是**情况3**。
	*   情况2、4、5都会通过旋转来终止修复循环。
	
	现在，我们可以构造一个反例来证明原命题是错误的： #反例  
	
	**设想一个场景**：我们删除一个黑色节点后，产生的双黑节点 `x`，其兄弟节点 `s` 是黑色的，且 `s` 的两个子节点也是黑色的（情况3）。于是我们对 `s` 进行重染色，并将双黑问题传递给 `x` 的父节点。假设新的双黑节点（即原 `x` 的父节点）的兄弟节点及其两个子节点也恰好都是黑色的。这个过程可以一直重复，双黑问题沿着一条路径一直向上传递，直到根节点。
	
	当双黑问题传递到根节点时，我们应用**情况1**，直接将根节点的双黑属性移除即可。
	
	在整个“情况3 -> 情况3 -> ... -> 情况1”的修复链条中，我们执行了多次（最坏情况下是 $O(\log n)$ 次）重染色，但**没有进行任何一次旋转**。
	
	因此，“进行了 $Ω(\log n)$ 次重染色”并不必然导致“至少旋转一次”。原命题错误

-  衍生 
	-  [[平衡二叉树与普通二叉树比较与红黑树比较]]   
		-   问题：AVL树和红黑树有什么区别？在什么场景下分别使用它们？
	    *   答案：AVL树是更严格的平衡二叉树（左右子树高度差不超过1），因此查找效率通常更高。但为了维持这种严格的平衡，其插入和删除操作可能需要更多的旋转，开销更大。红黑树的平衡性要求较宽松，插入删除开销较小。因此，如果应用场景中查找操作远多于插入删除，AVL树可能更优；如果插入删除操作频繁，红黑树是更好的选择。
	* #红黑树删除操作的复杂度   
		* 问题：红黑树删除一个 节点最多需要多少次旋转？
	    *   答案：最多3次。
	- 实际应用场景：   
		-  问题：哪些常见的数据结构或系统组件使用了红黑树？
	    *   答案：C++ STL中的 `map`, `set`, `multimap`, `multiset`；Java中的 `TreeMap`, `TreeSet`；Linux内核中的CFS调度器（Completely Fair Scheduler）等。  
	

![[Pasted image 20251112045325.png]]
- 这道题的核心是考察希尔排序（Shell Sort）在特定步长序列下的时间复杂度  
1. #希尔排序的基本思想    [[希尔排序]]   
	1.  通过将相距某个“增量”的记录组成一个子序列，对各个子序列分别进行直接插入排序，然后逐步缩减增量，重复这个过程，直到增量为1，完成最后一次整体的插入排序。   
2.  题目给出的步长序列是 `{1, 3, 7, 15, ..., 2^k - 1, ...}`。在实际使用中，这个序列是倒序使用的，即从最大的步长（小于数组长度n）开始，逐步递减到1。这个序列被称为 #Hibbard增量序列  
3. Hibbard序列的时间复杂度   
	1. 经过数学家的证明，当希尔排序采用Hibbard增量序列 $h_k = 2^k - 1$ 时，其最坏情况下的时间复杂度为 $O(n^{3/2})$。  
	2. 题目中给出的时间复杂度是 $O(n^{1.5})$。因为 $1.5 = 3/2$，所以 $O(n^{1.5})$ 和 $O(n^{3/2})$ 是等价的。因此，原题目的陈述是正确的   
4. 大步长组 ($h \ge \sqrt{n}$):
    对于这些非常大的步长，数组还很混乱，“部分有序”的特性不明显。因此，我们使用步骤一中的朴素成本公式 $O(n^2/h)$ 更为合适。
    在这个区间内，单次成本最高的情况发生在 $h$ 最小的时候，即当 $h \approx \sqrt{n}$ 时。此时的成本为：
    $$
    \text{最大单次成本} \approx O\left(\frac{n^2}{\sqrt{n}}\right) = O(n^{3/2})
    $$
    所有比 $\sqrt{n}$ 更大的步长，其成本 $O(n^2/h)$ 都会小于 $O(n^{3/2})$。因此，整个大步长组的总成本由这个瓶颈步骤决定，其量级为 $O(n^{3/2})$。 
5. 希尔排序的性能高度依赖于步长序列。
	-   优秀的步长序列（如Hibbard序列）能让前一步的排序结果（**$k$-有序**）极大地优化后一步的排序效率。
	-   对于Hibbard序列，其复杂度瓶颈出现在步长 $h \approx \sqrt{n}$ 的阶段，其成本为 $O(n^{3/2})$，这个成本主导了整个算法的最终复杂度。  

![[Pasted image 20251112045333.png]]
T  , 
-  [[伸展树]] 的核心操作是“伸展”（Splaying），即在访问（查找、插入、删除）一个节点后，通过一系列旋转操作将其移动到树的根部。这个过程的目的是优化后续访问，使得经常被访问的节点离根更近。  
1. 然而，单次伸展操作**并不保证**树会变得更平衡或高度更低。虽然在多次操作的“摊还”意义下，伸展树的效率很高，但单次操作可能会导致树的高度增加、不变或减少。   
	1. 高度增加的情况（ #反例 证明）   
		1. 假设我们有一棵简单的伸展树，结构如下：
    ```
      2
     / \
    1   3 
    ```
    此时树的高度为1（根节点高度为0）。现在，我们访问节点 `1`。根据伸展树的规则（节点 `1` 是根的左孩子，进行一次Zig旋转），节点 `1` 会被旋转到根部。
    
    旋转后的树变为：
    ```
      1
       \
        2
         \
          3
    ```
    可以看到，访问节点 `1` 并将其伸展到根部后，树的高度从1增加到了2。这个例子充分证明了“树的高度不一定降低”，甚至可能增加。

2.  **高度降低的情况**
    如果我们访问上面那棵退化的树的中间节点 `2`，它已经是根了，树结构不变，高度不变。如果我们访问节点 `3`，经过两次旋转后，树会变得更平衡，高度降低。

3. 存在高度增加或不变的情况，所以“不一定降低”这个描述是完全正确的。  

- 衍生 
	- 与其他平衡树的对比  
		-  **问题：** 伸展树与AVL树、红黑树在平衡策略和性能上有何不同？  
			- **平衡策略：** AVL树和红黑树是“悲观”策略，每次操作后都严格检查并恢复平衡，保证最坏情况下的性能。伸展树是“乐观”策略，不保证单次操作的性能，但通过伸展操作保证长期（摊还）性能。  
			-   **空间开销：** 伸展树不需要额外的存储空间（如AVL树的高度因子或红黑树的颜色位）。
	        *   **实现复杂度：** 伸展树的旋转逻辑相对固定，实现起来可能比红黑树更简单 
	- 具体操作模拟：  
		-    **问题：** 给定一棵伸展树，请画出访问某个特定节点后，经过伸展操作得到的新树的形态。
	    *   **考点：** 这直接考察对Zig, Zig-Zig, Zig-Zag三种旋转方式的理解和应用能力。  




![[Pasted image 20251112045340.png]]
[[Boyer-Moore（BM）算法]]  
1.  每次移动时，算法会分别计算这两个策略建议的移动位数，然后取其中 **较大者** 作为最终的移动距离。即：`shift = max(坏字符移动距离, 好后缀移动距离)`  
	1. 最好情况时间复杂度 $O(n/m)$   
		1. BM算法的最好情况时间复杂度为 $O(n/m)$。这种情况通常发生在每次不匹配后，模式串都可以向后移动 `m` 位（即模式串的整个长度）。这样，总的比较次数大约是 $n/m$ 次，每次比较最多 `m` 个字符，但由于我们只关心比较的轮数，所以复杂度为 $O(n/m)$。   
	2. 典型的最好情况是：模式串中的字符在文本串的对应窗口中完全不存在。
		例如：
		文本串 T = "AAAAAAAAAAAAAAAA"
		模式串 P = "BCDE"
	当 P 第一次与 T 的 "AAAA" 对齐时，从右向左比较，P 的最后一个字符 'E' 与 T 的 'A' 不匹配。此时，文本串中的这个 'A' 就是“坏字符”。
2. 分析仅使用 #好后缀策略 的情况
	1. 题目中的核心问题：如果只用好后缀策略，不用坏字符策略，还能保证 $O(n/m)$ 的最好情况吗？
	2. 答案是：**不能保证**。
	
	*   **好后缀策略的定义**：当发生不匹配时，我们关注已经匹配成功的部分，这部分被称为“好后缀”。好后缀策略的目标是找到模式串中 **除自身外**，最右边的另一个与好后缀相同子串，然后将模式串移动到那个位置对齐。如果不存在，则寻找模式串的最长前缀，该前缀也是好后缀的后缀，并进行对齐。
	
	*   **好后缀策略的局限性**：好后缀策略的移动距离完全取决于模式串自身的结构（是否存在重复的子串或前后缀）。它 **不关心** 导致不匹配的那个文本串字符（即坏字符）是什么。
3. 构造一个反例，来说明为什么仅靠好后缀策略无法保证大的移动：
	**反例：**
	*   文本串 T = "bbbbbbbbbbbb"
	*   模式串 P = "abbb" (m=4)
	2. **第一次对齐**：
    ```
    T: bbbb bbbbbbbb
    P: abbb
    ```
	从右向左比较，`b`=`b`, `b`=`b`, `b`=`b`。当比较到 P 的第一个字符 `a` 和 T 的 `b` 时，发生不匹配。
    *   **已匹配部分**：`bbb`，这就是“好后缀”
	 -  **仅用好后缀策略**：我们需要在 P (`abbb`) 中寻找 `bbb` 的另一次出现。在 `abbb` 中，除了末尾的 `bbb`，没有其他地方出现 `bbb`。接下来，我们寻找 P 的一个前缀，它同时是 `bbb` 的后缀。
	2. `bbb` 的后缀有 `b`, `bb`, `bbb`
	3.  P 的前缀有 `a`, `ab`, `abb`, `abbb`
	4. 最长的公共部分是空。在这种情况下，好后缀规则的移动距离通常是 `m`。这看起来不错
4. 






![[Pasted image 20251112045349.png]]



![[Pasted image 20251112045356.png]]



![[Pasted image 20251112045407.png]]




![[Pasted image 20251112045416.png]]



![[Pasted image 20251112045424.png]]



![[Pasted image 20251112045433.png]]




![[Pasted image 20251112045452.png]]




![[Pasted image 20251112045501.png]]
![[Pasted image 20251112045507.png]]





![[Pasted image 20251112045525.png]]



![[Pasted image 20251112045533.png]]



![[Pasted image 20251112045541.png]]



![[Pasted image 20251112045546.png]]



![[Pasted image 20251112045549.png]]



![[Pasted image 20251112045553.png]]



![[Pasted image 20251112045600.png]]



![[Pasted image 20251112045605.png]]



![[Pasted image 20251112045609.png]]



![[Pasted image 20251112045613.png]]



![[Pasted image 20251112045617.png]]



![[Pasted image 20251112045805.png]]



![[Pasted image 20251112045812.png]]



![[Pasted image 20251112045826.png]]



![[Pasted image 20251112045838.png]]



![[Pasted image 20251112045846.png]]



![[Pasted image 20251112045854.png]]



![[Pasted image 20251112045902.png]]



![[Pasted image 20251112050043.png]]



![[Pasted image 20251112050046.png]]



![[Pasted image 20251112050056.png]]



![[Pasted image 20251112050059.png]]



![[Pasted image 20251112050105.png]]



![[Pasted image 20251112050117.png]]



![[Pasted image 20251112050125.png]]



![[Pasted image 20251112050215.png]]



![[Pasted image 20251112050134.png]]



![[Pasted image 20251112050143.png]]



![[Pasted image 20251112050155.png]]



![[Pasted image 20251112050217.png]]



![[Pasted image 20251112050225.png]]



![[Pasted image 20251112050426.png]]



![[Pasted image 20251112050435.png]]



![[Pasted image 20251112050442.png]]



![[Pasted image 20251112050636.png]]



![[Pasted image 20251112050640.png]]



![[Pasted image 20251112050644.png]]



![[Pasted image 20251112050647.png]]



![[Pasted image 20251112050651.png]]



![[Pasted image 20251112050722.png]]



![[Pasted image 20251112050726.png]]











